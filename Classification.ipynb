{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youri\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from collections import Counter \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows ร 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= movie_reviews['review']\n",
    "X=text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majid(X):\n",
    "    corpus = []\n",
    "    for i in range(0, len(X)):\n",
    "        review = re.sub(r'[@%\\\\*=()/~#&\\+รก?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(X[i])) #remove punctuation\n",
    "        review = re.sub(r'\\d+',' ', str(X[i]))# remove number\n",
    "        review = review.lower() #lower case\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
    "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
    "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
    "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
    "        corpus.append(review)        \n",
    "#    return corpus        \n",
    "    #Tokenizing and Word Count  \n",
    "    words=[]\n",
    "    for i in range(len(corpus)):\n",
    "        words= nltk.word_tokenize(corpus[i])\n",
    "        #sentences.append(words)\n",
    "   \n",
    "    return words\n",
    "\n",
    "X = [[el] for el in X] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "sentences = Parallel(n_jobs=num_cores)(delayed(majid)(i) for i in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#Porter stemmer\n",
    "#############\n",
    "def porter_stemming(sentences):\n",
    "        porter = PorterStemmer()\n",
    "        #words = sent.split() \n",
    "        stemmed_words = [porter.stem(word) for word in sentences]\n",
    "        return stemmed_words\n",
    "def majid_porter(X):\n",
    "    sentences= porter_stemming(X)\n",
    "    gc.collect()\n",
    "    return sentences\n",
    " \n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "#X = [[el] for el in X]\n",
    "    \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "porter = Parallel(n_jobs=num_cores)(delayed(majid_porter)(i) for i in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#Snowball Stemmer\n",
    "########################\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import nltk\n",
    "def snowball_stemming(sentences):\n",
    "        sno = nltk.stem.SnowballStemmer('english')\n",
    "        #words = sent.split() \n",
    "        stemmed_words = [sno.stem(word) for word in sentences]\n",
    "        return stemmed_words\n",
    "\n",
    "def majid_snowball(X):\n",
    "    sentences= snowball_stemming(X)\n",
    "    gc.collect()\n",
    "    return sentences\n",
    " \n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "#X = [[el] for el in X]\n",
    "    \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "snowball = Parallel(n_jobs=num_cores)(delayed(majid_snowball)(i) for i in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#Lancaster Stemmer\n",
    "########################\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "import nltk\n",
    "def lancaster_stemming(sentences):\n",
    "    lancaster = LancasterStemmer()\n",
    "    #words = sent.split() \n",
    "    stemmed_words = [lancaster.stem(word) for word in sentences]\n",
    "    return stemmed_words\n",
    "\n",
    "def majid_lancaster(X):\n",
    "    sentences= lancaster_stemming(X)\n",
    "    gc.collect()\n",
    "    return sentences\n",
    " \n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "#X = [[el] for el in X]\n",
    "    \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "lancaster = Parallel(n_jobs=num_cores)(delayed(majid_lancaster)(i) for i in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#Lemmatization\n",
    "########################\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "def lemmaztization(sentences):\n",
    "    WNlemmatizer = WordNetLemmatizer()\n",
    "    #words = sent.split() \n",
    "    lemmas = [WNlemmatizer.lemmatize(word) for word in sentences]\n",
    "    return lemmas\n",
    "\n",
    "def majid_lemmatization(X):\n",
    "    sentences= lemmaztization(X)\n",
    "    gc.collect()\n",
    "    return sentences\n",
    " \n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "#X = [[el] for el in X]\n",
    "    \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "lemma = Parallel(n_jobs=num_cores)(delayed(majid_lemmatization)(i) for i in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_porter = [' '.join(x) for x in porter]\n",
    "#sent4= TreebankWordDetokenizer().detokenize([ i for i in [sent3]])\n",
    "\n",
    "#after preprocess our reviews we will store them in a new list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_snowball = [' '.join(x) for x in snowball]\n",
    "#sent4= TreebankWordDetokenizer().detokenize([ i for i in [sent3]])\n",
    "\n",
    "#after preprocess our reviews we will store them in a new list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lancaster = [' '.join(x) for x in lancaster]\n",
    "#sent4= TreebankWordDetokenizer().detokenize([ i for i in [sent3]])\n",
    "\n",
    "#after preprocess our reviews we will store them in a new list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lemma = [' '.join(x) for x in lemma]\n",
    "#sent4= TreebankWordDetokenizer().detokenize([ i for i in [sent3]])\n",
    "\n",
    "#after preprocess our reviews we will store them in a new list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one of the other review ha mention that after watch just oz episod you 'll be hook . they are right , as thi is exactli what happen with me.th first thing that struck me about oz wa it brutal and unflinch scene of violenc , which set in right from the word go . trust me , thi is not a show for the faint heart or timid . thi show pull no punch with regard to drug , sex or violenc . it is hardcor , in the classic use of the word.it is call oz as that is the nicknam given to the oswald maximum secur state penitentari . it focus mainli on emerald citi , an experiment section of the prison where all the cell have glass front and face inward , so privaci is not high on the agenda . em citi is home to mani .. aryan , muslim , gangsta , latino , christian , italian , irish and more .... so scuffl , death stare , dodgi deal and shadi agreement are never far away.i would say the main appeal of the show is due to the fact that it goe where other show would n't dare . forget pretti pictur paint for mainstream audienc , forget charm , forget romanc ... oz doe n't mess around . the first episod i ever saw struck me as so nasti it wa surreal , i could n't say i wa readi for it , but as i watch more , i develop a tast for oz , and got accustom to the high level of graphic violenc . not just violenc , but injustic ( crook guard who 'll be sold out for a nickel , inmat who 'll kill on order and get away with it , well manner , middl class inmat be turn into prison bitch due to their lack of street skill or prison experi ) watch oz , you may becom comfort with what is uncomfort view .... that if you can get in touch with your darker side .\n"
     ]
    }
   ],
   "source": [
    "print(X_porter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to convert our labels into digits    \n",
    "y = movie_reviews['sentiment']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))\n",
    "#Y = pd.get_dummies(df['Product']).values\n",
    "#print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We  use train_test_split method from the sklearn.model.selection\n",
    "Xp_train, Xp_test, yp_train, yp_test = train_test_split(X_porter, y, test_size=0.20, random_state=42) #porter\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_snowball, y, test_size=0.20, random_state=42) #snowball\n",
    "Xla_train, Xla_test, yla_train, yla_test = train_test_split(X_lancaster, y, test_size=0.20, random_state=42) #lancaster\n",
    "Xle_train, Xle_test, yle_train, yle_test = train_test_split(X_lemma, y, test_size=0.20, random_state=42) #snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Embedding Layer\n",
    "#use the Tokenizer class from the keras.preprocessing.\n",
    "#text module to create a word-to-index dictionary.\n",
    "#In the word-to-index dictionary, each word in the corpus is used as a key, \n",
    "#while a corresponding unique index is used as the value for the key\n",
    "def prepare_embedding(X_train, X_test):\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # Adding 1 because of reserved 0 index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocab size:',vocab_size)\n",
    "    maxlen = 100\n",
    "\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return X_train, X_test, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 78012\n",
      "Vocab size: 73690\n",
      "Vocab size: 72586\n",
      "Vocab size: 91915\n"
     ]
    }
   ],
   "source": [
    "Xp_train, Xp_test, vocab_size_p, tokenizer_p = prepare_embedding(Xp_train, Xp_test) #porter\n",
    "Xs_train, Xs_test, vocab_size_s, tokenizer_s = prepare_embedding(Xs_train, Xs_test) #snowball\n",
    "Xla_train, Xla_test, vocab_size_la, tokenizer_la = prepare_embedding(Xla_train, Xla_test) #lancaster\n",
    "Xle_train, Xle_test, vocab_size_le, tokenizer_le = prepare_embedding(Xle_train, Xle_test) #lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use pretrained embeddings to create our feature matrix. \n",
    "#In the following script we load the pretrained word embeddings \n",
    "#and create a dictionary that will contain words as keys and \n",
    "#their corresponding embedding list as values.\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def prepare_embedding_layer(embedding_file, vocab_size, tokenizer):\n",
    "    embeddings_dictionary = dict()\n",
    "    glove_file = open(embedding_file, encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = asarray(records[1:])\n",
    "        embeddings_dictionary [word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "    \n",
    "    embedding_matrix = zeros((vocab_size, 300))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_CBOWporter = prepare_embedding_layer(\"W-CBOW-Stem-porter.txt\", vocab_size_p, tokenizer_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_CBOWsnowball = prepare_embedding_layer(\"W-CBOW-Stem-snowball.txt\", vocab_size_s, tokenizer_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_CBOWlancaster = prepare_embedding_layer(\"W-CBOW-Stem-lancaster.txt\", vocab_size_la, tokenizer_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_CBOWlemma = prepare_embedding_layer(\"W-CBOW-Stem-lemma.txt\", vocab_size_le, tokenizer_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_SKIPporter = prepare_embedding_layer(\"W-Skip-Stem-porter.txt\", vocab_size_p, tokenizer_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_SKIPsnowball = prepare_embedding_layer(\"W-Skip-Stem-snowball.txt\", vocab_size_s, tokenizer_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_SKIPlancaster = prepare_embedding_layer(\"W-Skip-Stem-lancaster.txt\", vocab_size_la, tokenizer_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_SKIPlemma = prepare_embedding_layer(\"W-Skip-Stem-lemma.txt\", vocab_size_le, tokenizer_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the model\n",
    "\n",
    "def get_model(X_train, y_train, vocab_size, embedding_matrix, maxlen=100):\n",
    "\tmodel = Sequential()\n",
    "\tembedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "\tmodel.add(embedding_layer)\n",
    "\tmodel.add(LSTM(128))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# define model\n",
    "\t#model = Sequential()\n",
    "\t#model.add(Dense(100, input_dim=2, activation='relu'))\n",
    "\t#model.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\t# fit model\n",
    "\tmodel.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.10)\n",
    "\treturn model\n",
    "\n",
    "####################\n",
    "\n",
    "# compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# fit the model\n",
    "#history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.10)\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 58s 146ms/step - loss: 0.5576 - acc: 0.7017 - val_loss: 0.3990 - val_acc: 0.8142\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.3791 - acc: 0.8319 - val_loss: 0.3751 - val_acc: 0.8265\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 0.3079 - acc: 0.8694 - val_loss: 0.4038 - val_acc: 0.8092\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 39s 137ms/step - loss: 0.2497 - acc: 0.8981 - val_loss: 0.3686 - val_acc: 0.8397\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 0.2000 - acc: 0.9207 - val_loss: 0.3678 - val_acc: 0.8395\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.1565 - acc: 0.9452 - val_loss: 0.3925 - val_acc: 0.8453\n"
     ]
    }
   ],
   "source": [
    "model_CBOWporter = get_model(Xp_train, yp_train, vocab_size_p, embedding_matrix_CBOWporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 38s 127ms/step - loss: 0.5752 - acc: 0.6841 - val_loss: 0.4152 - val_acc: 0.8090\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 0.3690 - acc: 0.8405 - val_loss: 0.3634 - val_acc: 0.8375\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 35s 126ms/step - loss: 0.2964 - acc: 0.8756 - val_loss: 0.3508 - val_acc: 0.8435\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 36s 126ms/step - loss: 0.2407 - acc: 0.9051 - val_loss: 0.3478 - val_acc: 0.8455\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 0.1937 - acc: 0.9271 - val_loss: 0.3742 - val_acc: 0.8503\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 36s 126ms/step - loss: 0.1409 - acc: 0.9513 - val_loss: 0.3912 - val_acc: 0.8455\n"
     ]
    }
   ],
   "source": [
    "model_CBOWsnowball = get_model(Xs_train, ys_train, vocab_size_s, embedding_matrix_CBOWsnowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 39s 129ms/step - loss: 0.5723 - acc: 0.6911 - val_loss: 0.4189 - val_acc: 0.8048\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 35s 125ms/step - loss: 0.3826 - acc: 0.8322 - val_loss: 0.4051 - val_acc: 0.8165\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 36s 126ms/step - loss: 0.3210 - acc: 0.8633 - val_loss: 0.3674 - val_acc: 0.8347\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 0.2602 - acc: 0.8909 - val_loss: 0.4367 - val_acc: 0.8000\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 35s 124ms/step - loss: 0.2136 - acc: 0.9153 - val_loss: 0.3884 - val_acc: 0.8430\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 35s 124ms/step - loss: 0.1645 - acc: 0.9397 - val_loss: 0.4260 - val_acc: 0.8418\n"
     ]
    }
   ],
   "source": [
    "model_CBOWlancaster = get_model(Xla_train, yla_train, vocab_size_la, embedding_matrix_CBOWlancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 39s 129ms/step - loss: 0.5762 - acc: 0.6870 - val_loss: 0.4074 - val_acc: 0.8080\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 0.3769 - acc: 0.8373 - val_loss: 0.3882 - val_acc: 0.8142\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 0.3075 - acc: 0.8692 - val_loss: 0.3906 - val_acc: 0.8307\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 0.2461 - acc: 0.8995 - val_loss: 0.3551 - val_acc: 0.8457\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 0.1936 - acc: 0.9262 - val_loss: 0.3891 - val_acc: 0.8465\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 35s 124ms/step - loss: 0.1472 - acc: 0.9485 - val_loss: 0.3960 - val_acc: 0.8537\n"
     ]
    }
   ],
   "source": [
    "model_CBOWlemma = get_model(Xle_train, yle_train, vocab_size_le, embedding_matrix_CBOWlemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 38s 127ms/step - loss: 0.5833 - acc: 0.6722 - val_loss: 0.4283 - val_acc: 0.8015\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 35s 125ms/step - loss: 0.4278 - acc: 0.8038 - val_loss: 0.4673 - val_acc: 0.7688\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.3977 - acc: 0.8178 - val_loss: 0.4661 - val_acc: 0.8018\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.3625 - acc: 0.8428 - val_loss: 0.3494 - val_acc: 0.8512\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 37s 130ms/step - loss: 0.3271 - acc: 0.8579 - val_loss: 0.3545 - val_acc: 0.8508\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 39s 137ms/step - loss: 0.3071 - acc: 0.8655 - val_loss: 0.3267 - val_acc: 0.8645\n"
     ]
    }
   ],
   "source": [
    "model_SKIPporter = get_model(Xp_train, yp_train, vocab_size_p, embedding_matrix_SKIPporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 42s 138ms/step - loss: 0.5872 - acc: 0.6724 - val_loss: 0.5449 - val_acc: 0.7498\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 38s 137ms/step - loss: 0.4526 - acc: 0.7934 - val_loss: 0.4137 - val_acc: 0.8085\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 39s 137ms/step - loss: 0.4057 - acc: 0.8154 - val_loss: 0.4047 - val_acc: 0.8290\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.3655 - acc: 0.8359 - val_loss: 0.3753 - val_acc: 0.8260\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 41s 146ms/step - loss: 0.3419 - acc: 0.8472 - val_loss: 0.3648 - val_acc: 0.8393\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 0.3221 - acc: 0.8594 - val_loss: 0.3411 - val_acc: 0.8530\n"
     ]
    }
   ],
   "source": [
    "model_SKIPsnowball = get_model(Xs_train, ys_train, vocab_size_s, embedding_matrix_SKIPsnowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 40s 135ms/step - loss: 0.6069 - acc: 0.6649 - val_loss: 0.4464 - val_acc: 0.7925\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 0.4397 - acc: 0.7961 - val_loss: 0.4529 - val_acc: 0.7905\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 0.4134 - acc: 0.8117 - val_loss: 0.3979 - val_acc: 0.8195\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.3778 - acc: 0.8305 - val_loss: 0.3713 - val_acc: 0.8330\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 0.3480 - acc: 0.8456 - val_loss: 0.3746 - val_acc: 0.8450\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 0.3359 - acc: 0.8530 - val_loss: 0.4231 - val_acc: 0.8055\n"
     ]
    }
   ],
   "source": [
    "model_SKIPlancaster = get_model(Xla_train, yla_train, vocab_size_la, embedding_matrix_SKIPlancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "282/282 [==============================] - 41s 139ms/step - loss: 0.5721 - acc: 0.6904 - val_loss: 0.4222 - val_acc: 0.8055\n",
      "Epoch 2/6\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 0.4269 - acc: 0.8050 - val_loss: 0.4124 - val_acc: 0.8065\n",
      "Epoch 3/6\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 0.3893 - acc: 0.8239 - val_loss: 0.3716 - val_acc: 0.8360\n",
      "Epoch 4/6\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.3574 - acc: 0.8429 - val_loss: 0.3550 - val_acc: 0.8460\n",
      "Epoch 5/6\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 0.3224 - acc: 0.8618 - val_loss: 0.3594 - val_acc: 0.8457\n",
      "Epoch 6/6\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.2973 - acc: 0.8738 - val_loss: 0.3330 - val_acc: 0.8535\n"
     ]
    }
   ],
   "source": [
    "model_SKIPlemma = get_model(Xle_train, yle_train, vocab_size_le, embedding_matrix_SKIPlemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras import backend as K\\ndef f1(y_true, y_pred):\\n    def recall(y_test, y_pred):\\n        \"\"\"Recall metric.\\n        Only computes a batch-wise average of recall.\\n        Computes the recall, a metric for multi-label classification of\\n        how many relevant items are selected.\\n        \"\"\"\\n        true_positives = K.sum(K.round(K.clip(y_test * y_pred, 0, 1)))\\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\\n        recall = true_positives / (possible_positives + K.epsilon())\\n        return recall\\n    def precision(y_true, y_pred):\\n        \"\"\"Precision metric.\\n        Only computes a batch-wise average of precision.\\n        Computes the precision, a metric for multi-label classification of\\n        how many selected items are relevant.\\n        \"\"\"\\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\\n        precision = true_positives / (predicted_positives + K.epsilon())\\n        return precision\\n    precision = precision(y_true, y_pred)\\n    recall = recall(y_true, y_pred)\\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\\n####\\n    '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "from keras import backend as K\n",
    "def recall_m(y_train, y_test):\n",
    "        true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_train, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "def precision_m(y_train, y_test):\n",
    "        true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_test, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "def f1_m(y_train, y_test):\n",
    "    precision = precision_m(y_train, y_test)\n",
    "    recall = recall_m(y_train, y_test)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "'''\n",
    "'''\n",
    "from keras import backend as K\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_test, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_test * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "####\n",
    "    '''\n",
    "# compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# fit the model\n",
    "#history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "#y_pred = model.predict(X_test)\n",
    "#print(precision_recall_fscore_support(y_test, y_pred))\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    yhat_probs = model.predict(X_test, verbose=1)\n",
    "    # predict crisp classes for test set\n",
    "    yhat_classes = model.predict_classes(X_test, verbose=1)\n",
    "    #yhat_classes = np.argmax(model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    # reduce to 1d array\n",
    "    yhat_probs = yhat_probs[:, 0]\n",
    "    yhat_classes = yhat_classes[:, 0]\n",
    " \n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    print('F1 score: %f' % f1)\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, yhat_classes)\n",
    "    print(matrix)\n",
    "    \n",
    "    #print(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "    #print(classification_report(y_test, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "    # evaluate the model\n",
    "    #loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 22ms/step\n",
      "  5/313 [..............................] - ETA: 9s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youri\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 23ms/step\n",
      "Accuracy: 0.852600\n",
      "Precision: 0.872986\n",
      "Recall: 0.827942\n",
      "F1 score: 0.849868\n",
      "[[4354  607]\n",
      " [ 867 4172]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_CBOWporter, Xp_test, yp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 26ms/step\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "Accuracy: 0.850200\n",
      "Precision: 0.820221\n",
      "Recall: 0.899980\n",
      "F1 score: 0.858251\n",
      "[[3967  994]\n",
      " [ 504 4535]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_CBOWsnowball, Xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "Accuracy: 0.853100\n",
      "Precision: 0.830556\n",
      "Recall: 0.890058\n",
      "F1 score: 0.859278\n",
      "[[4046  915]\n",
      " [ 554 4485]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_CBOWlancaster, Xla_test, yla_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "Accuracy: 0.854600\n",
      "Precision: 0.845575\n",
      "Recall: 0.870411\n",
      "F1 score: 0.857813\n",
      "[[4160  801]\n",
      " [ 653 4386]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_CBOWlemma, Xle_test, yle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "Accuracy: 0.862700\n",
      "Precision: 0.850612\n",
      "Recall: 0.882516\n",
      "F1 score: 0.866271\n",
      "[[4180  781]\n",
      " [ 592 4447]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_SKIPporter, Xp_test, yp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "Accuracy: 0.857500\n",
      "Precision: 0.841201\n",
      "Recall: 0.884104\n",
      "F1 score: 0.862119\n",
      "[[4120  841]\n",
      " [ 584 4455]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_SKIPsnowball, Xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 23ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "Accuracy: 0.807400\n",
      "Precision: 0.743089\n",
      "Recall: 0.944235\n",
      "F1 score: 0.831673\n",
      "[[3316 1645]\n",
      " [ 281 4758]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_SKIPlancaster, Xla_test, yla_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "Accuracy: 0.857400\n",
      "Precision: 0.878483\n",
      "Recall: 0.832110\n",
      "F1 score: 0.854668\n",
      "[[4381  580]\n",
      " [ 846 4193]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_SKIPlemma, Xle_test, yle_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
